---
title: CMU Advanced NLP 2024 (6) Generation Algorithms
author: Sohyun
layout: post
---

위 강의에서는 여러 생성 알고리즘(Generation Algorithms)과, 이와 관련된 여러 이론을 설명하고 있다.

교수는 먼저 하나의 모델 $M$을 가정하고 강의를 시작한다.

해당 모델은 70억 개의 매개변수로 이루어져있고, 가장 최신의 아키텍쳐로 사전 학습된 모델이며, 수 조개의 텍스트 토큰으로 사전 학습되었다. 또한, 여러 리더보드에서 최고의 성능을 자랑하는 매우 유명한 모델이라고 말한다.

하지만 모델 $M$을 자세히 살펴보면, 사실 이 모델은 조건부 확률 분포를 정의한다. 모델이 조건부 확률 분포를 정의한다는 것은, 모델이 특정 조건 $X$가 주어졌을 때 다른 변수 $Y$의 분포를 예측하거나 설명할 수 있다는 의미이다. 모델에 입력 $X$를 넣으면, 관심 있는 시퀀스에 대한 확률을 출력한다. 특히, $M$은 어휘의 모든 토큰에 대한 확률 분포를 제공하여 다음에 어떤 토큰을 출력할지 예측한다. 이러한 의미를 아래의 식이 내포한다.

$$P(Y | X) = \prod_{j=1}^{J} P(y_j | X, y_1, \ldots, y_{j-1})$$

•  **입력과 예측**: 입력 $X$와 지금까지 예측한 모든 것을 기반으로 다음 토큰 $y_j$의 확률을 제공한다.

•  **확률 계산**: 시퀀스 내 모든 확률을 곱하면, 입력 $X$에 대한 출력 $Y$의 확률을 계산할 수 있다.

즉, 이 고급 모델은 단순히 조건부 확률 분포에 불과하다. 그럼에도 불구하고, 이를 활용하여 번역, 요약, 추론 등 다양한 NLP 작업을 수행할 수 있다. 입력 X와 출력 Y의 정의를 바꾸는 것만으로도 다양한 작업에 적용할 수 있다. 아래는 그 예시이다.


![제목 없음](https://github.com/user-attachments/assets/f49d05b4-2125-4730-916c-e4b2280b2ea6){: .responsive-img .align-center}



모델이 단순한 확률 분포로 작동할 때의 좋은 점은 모델이 예측의 확신도(confidence)를 제공할 수 있다는 것이다. 예를 들어 "2 + 2 = ?"라는 입력에 대해 모델이 '4'에 높은 확률을 부여하면, 모델이 매우 확신하고 있음을 알 수 있다. 반면, "교수가 좋아하는 색깔은?" 같은 질문에 대해서는 분포가 평평하게 나타나 확신도가 낮음을 알 수 있다.


![2](https://github.com/user-attachments/assets/2b8033f2-0f0e-4a86-af23-65dfb49dea94){: .responsive-img .align-center}


그러나 이러한 확률 분포 모델은 잘못된 출력을 생성할 수도 있다. 모델은 잘못된 단어에도 0이 아닌 적은 확률을 할당하기 때문에 예측 시, 잘못되거나 이상한 출력을 생성할 가능성이 있다. 이러한 문제는 모델이 훈련된 데이터가 완벽하더라도 발생할 수 있다. 이러한 문제를 할루시네이션(Hallucination)이라고 한다.

이러한 문제를 해결할 방법으로 해당 강의에서는 **샘플링**을 소개한다.


## 샘플링(Sampling for LMs)

모델에서 좋은 출력을 얻기 위한 방법 중 하나가 바로 샘플링이다. 다양한 샘플링 기법들이 있으며, 각 방법은 모델의 확률 분포에서 토큰을 선택하는 방법에 따라 다르다.

첫 번째로 소개하는 것은 단순 샘플링(Ancestral Sampling)이다.

### 단순 샘플링(Ancestral Sampling)

단순 샘플링은 단순히 확률 분포에 따라 토큰을 무작위로 선택하는 방법이다. 확률이 높은 토큰일수록 고를 확률이 높기 때문에 당연히 선택될 확률도 높다. 각 시간 단계(time step)마다 토큰을 하나씩 샘플링하며, 모델 분포에 따라 샘플링하기 때문에 분포에 맞는 샘플, 즉, 많은 샘플을 선택하다 보면 모델 분포와 거의 일치하는 결과를 얻을 수 있다. 이로 인해, 확률이 낮은 토큰도 선택 가능성이 있으며, 다양한 결과를 얻을 수 있다. 하지만 이 샘플링은 긴 꼬리 분포 문제를 겪게 된다.

**긴 꼬리 분포 문제란?** 

긴 꼬리 분포는 높은 확률을 가진 토큰이 일부이고, 많은 토큰들이 낮은 확률을 가지지만, 이들이 모여서 전체 확률의 절반 가까이를 차지하는 모양의 분포를 의미한다. 

![3](https://github.com/user-attachments/assets/98fbf138-489d-4ae9-aa56-ff9ac794a723){: .responsive-img .align-center}

대충 이런 모양의 분포이다.

이러한 분포의 문제는 결과적으로, 샘플링할 때 이러한 낮은 확률의 토큰이 선택될 가능성이 높다는 것이다. 위의 그림에서 녹색 부분과 노란 부분은 각각 절반 정도의 비율을 차지하는데, 녹색 부분이 더 정답에 가까운 친구들임에도 단순 샘플링을 통해 샘플링을 진행할 경우, 50%의 확률로 노란 부분을 선택하게 된다는 것이다. 

이런 문제를 해결하기 위해 고안된 것 중 하나가  바로 Top-k 샘플링이다.


### Top-k 샘플링

이 방법은 상위 K개의 토큰, 즉 가장 확률이 높은 K개의 토큰만 선택하여 이들 중에서 샘플링하는 것이다. K의 경우 사용자가 지정할 수 있다. 예를 들어 K가 6일 경우, 상위 6개의 토큰 중에서 샘플링을 하는 것이다.

이 경우 위와 같은 긴 꼬리 분포를 띄는 상황에서 확률 질량의 대부분을 차지하는 토큰들만 샘플링하여 긴 꼬리의 영향을 줄일 수 있다. 하지만 때로는 상위 K개의 토큰만으로 충분하지 않을 수 있고, 만약 분포가 평평할 경우, 상위 K개의 토큰이 전체 확률의 대부분을 차지하지 않을 수 있다.

### Top-P(or Nucleus) 샘플링

이 방법은 
<!--stackedit_data:
eyJoaXN0b3J5IjpbNzY3Mzc3Mzk3LC0xOTM5NDU5MTU5LC0xMz
AzNzY1NzkzLC05MDE4NjQzNTUsMTc5NDA1ODIxNCwtMTA5Njc5
MjYwOCwtMTk1ODA2NTI1LDM0NTAyNjg1OSwtMTc0MjkzMTU3Ni
w1MzM5ODU0NTgsMTA3OTE0NTEyMCwtMTMwNjE3MDAwNiw1ODgy
MjEwMCwtMTIwNjQ5MjY3MywtNDc0Mjg5MTk4LC0xMDIxMzE5Nj
Q1LDUxNTM2MzMyMiwxMDg5OTU0NzcyXX0=
-->