---
title: CMU Advanced NLP 2024 (6) Generation Algorithms
author: Sohyun
layout: post
---

위 강의에서는 여러 생성 알고리즘(Generation Algorithms)과, 이와 관련된 여러 이론을 설명하고 있다.

교수는 먼저 하나의 모델 $M$을 가정하고 강의를 시작한다.

해당 모델은 70억 개의 매개변수로 이루어져있고, 가장 최신의 아키텍쳐로 사전 학습된 모델이며, 수 조개의 텍스트 토큰으로 사전 학습되었다. 또한, 여러 리더보드에서 최고의 성능을 자랑하는 매우 유명한 모델이라고 말한다.

하지만 모델 $M$을 자세히 살펴보면, 사실 이 모델은 조건부 확률 분포를 정의한다. 모델이 조건부 확률 분포를 정의한다는 것은, 모델이 특정 조건 $X$가 주어졌을 때 다른 변수 $Y$의 분포를 예측하거나 설명할 수 있다는 의미이다. 모델에 입력 $X$를 넣으면, 관심 있는 시퀀스에 대한 확률을 출력한다. 특히, $M$은 어휘의 모든 토큰에 대한 확률 분포를 제공하여 다음에 어떤 토큰을 출력할지 예측한다. 이러한 의미를 아래의 식이 내포한다.

$$P(Y | X) = \prod_{j=1}^{J} P(y_j | X, y_1, \ldots, y_{j-1})$$

•  **입력과 예측**: 입력 $X$와 지금까지 예측한 모든 것을 기반으로 다음 토큰 $y_j$의 확률을 제공한다.

•  **확률 계산**: 시퀀스 내 모든 확률을 곱하면, 입력 $X$에 대한 출력 $Y$의 확률을 계산할 수 있다.

즉, 이 고급 모델은 단순히 조건부 확률 분포에 불과하다. 그럼에도 불구하고, 이를 활용하여 번역, 요약, 추론 등 다양한 NLP 작업을 수행할 수 있다. 입력 X와 출력 Y의 정의를 바꾸는 것만으로도 다양한 작업에 적용할 수 있다. 아래는 그 예시이다.


![제목 없음](https://github.com/user-attachments/assets/f49d05b4-2125-4730-916c-e4b2280b2ea6){: .responsive-img .align-center}



모델이 단순한 확률 분포로 작동할 때의 좋은 점은 모델이 예측의 확신도(confidence)를 제공할 수 있다는 것이다. 예를 들어 "2 + 2 = ?"라는 입력에 대해 모델이 '4'에 높은 확률을 부여하면, 모델이 매우 확신하고 있음을 알 수 있다. 반면, "교수가 좋아하는 색깔은?" 같은 질문에 대해서는 분포가 평평하게 나타나 확신도가 낮음을 알 수 있다.


![2](https://github.com/user-attachments/assets/2b8033f2-0f0e-4a86-af23-65dfb49dea94){: .responsive-img .align-center}


그러나 이러한 확률 분포 모델은 잘못된 출력을 생성할 수도 있다. 모델은 잘못된 단어에도 0이 아닌 적은 확률을 할당하기 때문에 예측 시, 잘못되거나 이상한 출력을 생성할 가능성이 있다. 이러한 문제는 모델이 훈련된 데이터가 완벽하더라도 발생할 수 있다. 이러한 문제를 할루시네이션(Hallucination)이라고 한다.

이러한 문제를 해결할 방법으로 해당 강의에서는 **샘플링**을 소개한다.


## 샘플링(Sampling for LMs)

모델에서 좋은 출력을 얻기 위한 방법 중 하나가 바로 샘플링이다. 다양한 샘플링 기법들이 있으며, 각 방법은 모델의 확률 분포에서 토큰을 선택하는 방법에 따라 다르다.

첫 번째로 소개하는 것은 단순 샘플링(Ancestral Sampling)이다.

### 단순 샘플링(Ancestral Sampling)

단순 샘플링은 단순히 확률 분포에 따라 토큰을 무작위로 선택하는 방법이다. 확률이 높은 토큰일수록 고를 확률이 높기 때문에 당연히 선택될 확률도 높다. 각 시간 단계(time step)마다 토큰을 하나씩 샘플링하며, 모델 분포에 따라 샘플링하기 때문에 분포에 맞는 샘플, 즉, 많은 샘플을 선택하다 보면 모델 분포와 거의 일치하는 결과를 얻을 수 있다. 이로 인해, 확률이 낮은 토큰도 선택 가능성이 있으며, 다양한 결과를 얻을 수 있다. 하지만 이 샘플링은 긴 꼬리 분포 문제를 겪게 된다.

**긴 꼬리 분포 문제란?** 

긴 꼬리 분포는 높은 확률을 가진 토큰이 일부이고, 많은 토큰들이 낮은 확률을 가지지만, 이들이 모여서 전체 확률의 절반 가까이를 차지하는 모양의 분포를 의미한다. 

![3](https://github.com/user-attachments/assets/98fbf138-489d-4ae9-aa56-ff9ac794a723){: .responsive-img .align-center}

대충 이런 모양의 분포이다.

이러한 분포의 문제는 결과적으로, 샘플링할 때 이러한 낮은 확률의 토큰이 선택될 가능성이 높다는 것이다. 위의 그림에서 녹색 부분과 노란 부분은 각각 절반 정도의 비율을 차지하는데, 녹색 부분이 더 정답에 가까운 친구들임에도 단순 샘플링을 통해 샘플링을 진행할 경우, 50%의 확률로 노란 부분을 선택하게 된다는 것이다. 

이런 문제를 해결하기 위해 고안된 것 중 하나가  바로 Top-k 샘플링이다.


### Top-k 샘플링

이 방법은 상위 K개의 토큰, 즉 가장 확률이 높은 K개의 토큰만 선택하여 이들 중에서 샘플링하는 것이다. K의 경우 사용자가 지정할 수 있다. 예를 들어 K가 6일 경우, 상위 6개의 토큰 중에서 샘플링을 하는 것이다.

이 경우 위와 같은 긴 꼬리 분포를 띄는 상황에서 확률 질량의 대부분을 차지하는 토큰들만 샘플링하여 긴 꼬리의 영향을 줄일 수 있다. 하지만 때로는 상위 K개의 토큰만으로 충분하지 않을 수 있고, 만약 분포가 평평할 경우, 상위 K개의 토큰이 전체 확률의 대부분을 차지하지 않을 수 있다.

### Top-P(or Nucleus) 샘플링

이 방법은 샘플링할 집합을 뽑을 때, 갯수가 아닌 확률 질량의 총합을 기준으로 뽑는 방법이다. 여기서 p가 해당 확률을 나타내는 값이고, 만약 p 가 0.94 즉, 94%라면 가장 확률이 높은 토큰부터 시작해, 누적 확률이 94%에 도달할 때까지 집합에 토큰을 추가한 후, 해당 집합에서 샘플링을 진행하는 방법이다. 이 방법의 장점은 분포의 형태에 따라 동적으로 토큰 집합을 구성하여, 상위 K 샘플링보다 유연하게 작동한다는 것이다. 또한 정말 필요없는 토큰들을 확실히 배제할 수 있다. 하지만 상황에 따라 확률 질량의 누적 합이 P에 도달하기 전까지 많은 토큰을 포함해야 할 수 있다.


### 앱실론 $\epsilon$ 샘플링

앱실론 샘플링의 경우 최소 확률이 $\epsilon$ 이상인 토큰들만 샘플링하는 방법이다. 즉 만약 $\epsilon$이 0.05 일 경우, 해당 확률을 넘는 토큰들만 샘플링 대상에 포함하는 것이다. 이렇게 함으로써 확률이 매우 낮은 토큰을 배제하여 샘플링의 품질을 높일 수 있다. 하지만 반대로 낮은 확률의 토큰이 필요한 경우(즉 확률이 낮지만 정답일 경우), 이를 배제해버릴 수 있다.


샘플링에 있어 긴 꼬리 분포만이 고려되어야 할 점은 아니다.  **온도** 또한 주목하여야 한다. 확률 분포에서 온도는 샘플링 시 모델의 확률 분포를 조정하여 생성되는 텍스트의 다양성과 결정ㅅ
<!--stackedit_data:
eyJoaXN0b3J5IjpbMTI4MDI2NjM0OSw4NzgxNjQ2MDksNjYzMj
kwMzgzLC0xMDk5NjM4NDU3LC0yMjUxMzY0MjAsNzY3Mzc3Mzk3
LC0xOTM5NDU5MTU5LC0xMzAzNzY1NzkzLC05MDE4NjQzNTUsMT
c5NDA1ODIxNCwtMTA5Njc5MjYwOCwtMTk1ODA2NTI1LDM0NTAy
Njg1OSwtMTc0MjkzMTU3Niw1MzM5ODU0NTgsMTA3OTE0NTEyMC
wtMTMwNjE3MDAwNiw1ODgyMjEwMCwtMTIwNjQ5MjY3MywtNDc0
Mjg5MTk4XX0=
-->